"use strict";(self.webpackChunkdeep_notes=self.webpackChunkdeep_notes||[]).push([[70357],{907738:(e,a,t)=>{t.r(a),t.d(a,{assets:()=>l,contentTitle:()=>o,default:()=>d,frontMatter:()=>r,metadata:()=>n,toc:()=>c});const n=JSON.parse('{"id":"technologies/apache/apache-flink","title":"Apache Flink","description":"Apache Flink is a stream processing framework that can also handle batch tasks. It considers batches to simply be data streams with finite boundaries, and thus treats batch processing as a subset of stream processing. This stream-first approach to all processing has a number of interesting side effects.","source":"@site/docs/technologies/apache/apache-flink.md","sourceDirName":"technologies/apache","slug":"/technologies/apache/apache-flink","permalink":"/technologies/apache/apache-flink","draft":false,"unlisted":false,"editUrl":"https://github.com/rajacsp/rajacsp.github.io/tree/master/docs/technologies/apache/apache-flink.md","tags":[],"version":"current","lastUpdatedBy":"Deeapak Sood","lastUpdatedAt":1764576719000,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Apache","permalink":"/technologies/apache/"},"next":{"title":"Apache HBase","permalink":"/technologies/apache/apache-hbase"}}');var s=t(474848),i=t(28453);const r={},o="Apache Flink",l={},c=[{value:"Stream Processing Model",id:"stream-processing-model",level:3},{value:"Batch Processing Model",id:"batch-processing-model",level:3},{value:"Advantages and Limitations",id:"advantages-and-limitations",level:3},{value:"Summary",id:"summary",level:3},{value:"Getting Started",id:"getting-started",level:2},{value:"Source Tables",id:"source-tables",level:3},{value:"WaterMarks",id:"watermarks",level:2},{value:"Apache Flink- Stateful Computations over Data Streams",id:"apache-flink--stateful-computations-over-data-streams",level:2},{value:"PyFlink",id:"pyflink",level:2},{value:"Links",id:"links",level:3},{value:"Amazon Managed Service for Apache Flink",id:"amazon-managed-service-for-apache-flink",level:2},{value:"Links",id:"links-1",level:2}];function h(e){const a={a:"a",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(a.header,{children:(0,s.jsx)(a.h1,{id:"apache-flink",children:"Apache Flink"})}),"\n",(0,s.jsx)(a.p,{children:"Apache Flink is a stream processing framework that can also handle batch tasks. It considers batches to simply be data streams with finite boundaries, and thus treats batch processing as a subset of stream processing. This stream-first approach to all processing has a number of interesting side effects."}),"\n",(0,s.jsxs)(a.p,{children:["This stream-first approach has been called the ",(0,s.jsx)(a.strong,{children:"Kappa architecture"}),", in contrast to the more widely known Lambda architecture (where batching is used as the primary processing method with streams used to supplement and provide early but unrefined results). Kappa architecture, where streams are used for everything, simplifies the model and has only recently become possible as stream processing engines have grown more sophisticated."]}),"\n",(0,s.jsx)(a.h3,{id:"stream-processing-model",children:"Stream Processing Model"}),"\n",(0,s.jsx)(a.p,{children:"Flink's stream processing model handles incoming data on an item-by-item basis as a true stream. Flink provides its DataStream API to work with unbounded streams of data. The basic components that Flink works with are:"}),"\n",(0,s.jsxs)(a.ul,{children:["\n",(0,s.jsxs)(a.li,{children:[(0,s.jsx)(a.strong,{children:"Streams"})," are immutable, unbounded datasets that flow through the system"]}),"\n",(0,s.jsxs)(a.li,{children:[(0,s.jsx)(a.strong,{children:"Operators"})," are functions that operate on data streams to produce other streams"]}),"\n",(0,s.jsxs)(a.li,{children:[(0,s.jsx)(a.strong,{children:"Sources"})," are the entry point for streams entering the system"]}),"\n",(0,s.jsxs)(a.li,{children:[(0,s.jsx)(a.strong,{children:"Sinks"})," are the place where streams flow out of the Flink system. They might represent a database or a connector to another system"]}),"\n"]}),"\n",(0,s.jsx)(a.p,{children:"Stream processing tasks take snapshots at set points during their computation to use for recovery in case of problems. For storing state, Flink can work with a number of state backends depending with varying levels of complexity and persistence."}),"\n",(0,s.jsx)(a.p,{children:'Additionally, Flink\'s stream processing is able to understand the concept of "event time", meaning the time that the event actually occurred, and can handle sessions as well. This means that it can guarantee ordering and grouping in some interesting ways.'}),"\n",(0,s.jsx)(a.h3,{id:"batch-processing-model",children:"Batch Processing Model"}),"\n",(0,s.jsx)(a.p,{children:"Flink's batch processing model in many ways is just an extension of the stream processing model. Instead of reading from a continuous stream, it reads a bounded dataset off of persistent storage as a stream. Flink uses the exact same runtime for both of these processing models."}),"\n",(0,s.jsx)(a.p,{children:"Flink offers some optimizations for batch workloads. For instance, since batch operations are backed by persistent storage, Flink removes snapshotting from batch loads. Data is still recoverable, but normal processing completes faster."}),"\n",(0,s.jsx)(a.p,{children:"Another optimization involves breaking up batch tasks so that stages and components are only involved when needed. This helps Flink play well with other users of the cluster. Preemptive analysis of the tasks gives Flink the ability to also optimize by seeing the entire set of operations, the size of the data set, and the requirements of steps coming down the line."}),"\n",(0,s.jsx)(a.h3,{id:"advantages-and-limitations",children:"Advantages and Limitations"}),"\n",(0,s.jsx)(a.p,{children:"Flink is currently a unique option in the processing framework world. While Spark performs batch and stream processing, its streaming is not appropriate for many use cases because of its micro-batch architecture. Flink's stream-first approach offers low latency, high throughput, and real entry-by-entry processing."}),"\n",(0,s.jsx)(a.p,{children:"Flink manages many things by itself. Somewhat unconventionally, it manages its own memory instead of relying on the native Java garbage collection mechanisms for performance reasons. Unlike Spark, Flink does not require manual optimization and adjustment when the characteristics of the data it processes change. It handles data partitioning and caching automatically as well."}),"\n",(0,s.jsx)(a.p,{children:'Flink analyzes its work and optimizes tasks in a number of ways. Part of this analysis is similar to what SQL query planners do within relationship databases, mapping out the most effective way to implement a given task. It is able to parallelize stages that can be completed in parallel, while bringing data together for blocking tasks. For iterative tasks, Flink attempts to do computation on the nodes where the data is stored for performance reasons. It can also do "delta iteration", or iteration on only the portions of data that have changes.'}),"\n",(0,s.jsx)(a.p,{children:"In terms of user tooling, Flink offers a web-based scheduling view to easily manage tasks and view the system. Users can also display the optimization plan for submitted tasks to see how it will actually be implemented on the cluster. For analysis tasks, Flink offers SQL-style querying, graph processing and machine learning libraries, and in-memory computation."}),"\n",(0,s.jsx)(a.p,{children:"Flink operates well with other components. It is written to be a good neighbor if used within a Hadoop stack, taking up only the necessary resources at any given time. It integrates with YARN, HDFS, and Kafka easily. Flink can run tasks written for other processing frameworks like Hadoop and Storm with compatibility packages."}),"\n",(0,s.jsx)(a.p,{children:"One of the largest drawbacks of Flink at the moment is that it is still a very young project. Large scale deployments in the wild are still not as common as other processing frameworks and there hasn't been much research into Flink's scaling limitations. With the rapid development cycle and features like the compatibility packages, there may begin to be more Flink deployments as organizations get the chance to experiment with it."}),"\n",(0,s.jsx)(a.h3,{id:"summary",children:"Summary"}),"\n",(0,s.jsx)(a.p,{children:"Flink offers both low latency stream processing with support for traditional batch tasks. Flink is probably best suited for organizations that have heavy stream processing requirements and some batch-oriented tasks. Its compatibility with native Storm and Hadoop programs, and its ability to run on a YARN-managed cluster can make it easy to evaluate. Its rapid development makes it worth keeping an eye on."}),"\n",(0,s.jsx)(a.h2,{id:"getting-started",children:"Getting Started"}),"\n",(0,s.jsx)(a.p,{children:(0,s.jsx)(a.a,{href:"https://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/sql/gettingstarted/",children:"Getting Started | Apache Flink"})}),"\n",(0,s.jsx)(a.h3,{id:"source-tables",children:"Source Tables"}),"\n",(0,s.jsx)(a.p,{children:"As with all SQL engines, Flink queries operate on top of tables. It differs from a traditional database because Flink does not manage data at rest locally; instead, its queries operate continuously over external tables."}),"\n",(0,s.jsxs)(a.p,{children:["Flink data processing pipelines begin with source tables. Source tables produce rows operated over during the query\u2019s execution; they are the tables referenced in the\xa0",(0,s.jsx)(a.code,{children:"FROM"}),"\xa0clause of a query. These could be Kafka topics, databases, filesystems, or any other system that Flink knows how to consume."]}),"\n",(0,s.jsxs)(a.p,{children:["Tables can be defined through the SQL client or using environment config file. The SQL client support\xa0",(0,s.jsx)(a.a,{href:"https://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/sql/overview/",children:"SQL DDL commands"}),"\xa0similar to traditional SQL. Standard SQL DDL is used to\xa0",(0,s.jsx)(a.a,{href:"https://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/sql/create/",children:"create"}),",\xa0",(0,s.jsx)(a.a,{href:"https://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/sql/alter/",children:"alter"}),",\xa0",(0,s.jsx)(a.a,{href:"https://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/sql/drop/",children:"drop"}),"\xa0tables."]}),"\n",(0,s.jsxs)(a.p,{children:["Flink has a support for different\xa0",(0,s.jsx)(a.a,{href:"https://nightlies.apache.org/flink/flink-docs-master/docs/connectors/table/overview/",children:"connectors"}),"\xa0and\xa0",(0,s.jsx)(a.a,{href:"https://nightlies.apache.org/flink/flink-docs-master/docs/connectors/table/formats/overview/",children:"formats"}),"\xa0that can be used with tables. Following is an example to define a source table backed by a\xa0",(0,s.jsx)(a.a,{href:"https://nightlies.apache.org/flink/flink-docs-master/docs/connectors/table/formats/csv/",children:"CSV file"}),"\xa0with\xa0",(0,s.jsx)(a.code,{children:"emp_id"}),",\xa0",(0,s.jsx)(a.code,{children:"name"}),",\xa0",(0,s.jsx)(a.code,{children:"dept_id"}),"\xa0as columns in a\xa0",(0,s.jsx)(a.code,{children:"CREATE"}),"\xa0table statement."]}),"\n",(0,s.jsx)(a.pre,{children:(0,s.jsx)(a.code,{className:"language-sql",children:"SHOW FUNCTIONS;\n\nCREATE TABLE employee_information (\n    emp_id INT,\n    name VARCHAR,\n    dept_id INT\n) WITH (\n    'connector' = 'filesystem',\n    'path' = '/path/to/something.csv',\n    'format' = 'csv'\n);\n\nSELECT * from employee_information WHERE dept_id = 1;\n"})}),"\n",(0,s.jsx)(a.h2,{id:"watermarks",children:"WaterMarks"}),"\n",(0,s.jsxs)(a.ul,{children:["\n",(0,s.jsx)(a.li,{children:(0,s.jsx)(a.a,{href:"https://medium.com/@ipolyzos_/understanding-watermarks-in-apache-flink-c8793a50fbb8",children:"Understanding Watermarks in Apache Flink | by Giannis Polyzos | Medium"})}),"\n",(0,s.jsx)(a.li,{children:(0,s.jsx)(a.a,{href:"https://nightlies.apache.org/flink/flink-docs-master/docs/dev/datastream-v2/watermark/",children:"Watermark | Apache Flink"})}),"\n",(0,s.jsx)(a.li,{children:(0,s.jsx)(a.a,{href:"https://youtu.be/sdhwpUAjqaI",children:"Event Time and Watermarks | Apache Flink 101 - YouTube"})}),"\n",(0,s.jsx)(a.li,{children:(0,s.jsx)(a.a,{href:"https://youtu.be/j1Ud8blbMKo",children:"Windowing and Watermarks in Flink | Flink with Java - YouTube"})}),"\n"]}),"\n",(0,s.jsx)(a.h2,{id:"apache-flink--stateful-computations-over-data-streams",children:"Apache Flink- Stateful Computations over Data Streams"}),"\n",(0,s.jsx)(a.p,{children:(0,s.jsx)(a.img,{alt:"image",src:t(344267).A+"",width:"999",height:"328"})}),"\n",(0,s.jsxs)(a.p,{children:["Apache Flink is a framework and distributed processing engine for stateful computations over ",(0,s.jsx)(a.em,{children:"unbounded and bounded"})," data streams. Flink has been designed to run in ",(0,s.jsx)(a.em,{children:"all common cluster environments"}),", perform computations at ",(0,s.jsx)(a.em,{children:"in-memory speed"})," and at ",(0,s.jsx)(a.em,{children:"any scale"}),"."]}),"\n",(0,s.jsx)(a.p,{children:"Streaming dataflow engine for Java"}),"\n",(0,s.jsx)(a.h2,{id:"pyflink",children:"PyFlink"}),"\n",(0,s.jsx)(a.p,{children:"PyFlink is a Python API for Apache Flink that allows you to build scalable batch and streaming workloads, such as real-time data processing pipelines, large-scale exploratory data analysis, Machine Learning (ML) pipelines and ETL processes. If you\u2019re already familiar with Python and libraries such as Pandas, then PyFlink makes it simpler to leverage the full capabilities of the Flink ecosystem. Depending on the level of abstraction you need, there are two different APIs that can be used in PyFlink:"}),"\n",(0,s.jsxs)(a.ul,{children:["\n",(0,s.jsxs)(a.li,{children:["The\xa0",(0,s.jsx)(a.strong,{children:"PyFlink Table API"}),"\xa0allows you to write powerful relational queries in a way that is similar to using SQL or working with tabular data in Python."]}),"\n",(0,s.jsxs)(a.li,{children:["At the same time, the\xa0",(0,s.jsx)(a.strong,{children:"PyFlink DataStream API"}),"\xa0gives you lower-level control over the core building blocks of Flink,\xa0",(0,s.jsx)(a.a,{href:"https://nightlies.apache.org/flink/flink-docs-master/docs/concepts/stateful-stream-processing/",children:"state"}),"\xa0and\xa0",(0,s.jsx)(a.a,{href:"https://nightlies.apache.org/flink/flink-docs-master/docs/concepts/time/",children:"time"}),", to build more complex stream processing use cases."]}),"\n"]}),"\n",(0,s.jsx)(a.h3,{id:"links",children:"Links"}),"\n",(0,s.jsxs)(a.ul,{children:["\n",(0,s.jsx)(a.li,{children:(0,s.jsx)(a.a,{href:"https://pyflink.readthedocs.io/en/main/index.html",children:"PyFlink Docs \u2014 pyflink-docs version master"})}),"\n",(0,s.jsx)(a.li,{children:(0,s.jsx)(a.a,{href:"https://diptimanrc.medium.com/list/pyflink-kafka-getting-started-table-api-udfs-and-more-ff5bf8d9d41a",children:"List: Pyflink Kafka Getting Started - Table API, UDFs and more | Curated by Diptiman Raichaudhuri | Medium"})}),"\n",(0,s.jsxs)(a.li,{children:[(0,s.jsx)(a.a,{href:"https://github.com/diptimanr/kafka_flink_getting_started",children:"GitHub - diptimanr/kafka_flink_getting_started: pyflink Table API and FLink SQL on Kafka"}),"\n",(0,s.jsxs)(a.ul,{children:["\n",(0,s.jsx)(a.li,{children:(0,s.jsx)(a.a,{href:"https://github.com/diptimanr/kafka_flink_getting_started/blob/master/06_kafka_pyflink_tableapi_tumbling_window.py",children:"kafka_flink_getting_started/06_kafka_pyflink_tableapi_tumbling_window.py at master \xb7 diptimanr/kafka_flink_getting_started \xb7 GitHub"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(a.h2,{id:"amazon-managed-service-for-apache-flink",children:"Amazon Managed Service for Apache Flink"}),"\n",(0,s.jsx)(a.p,{children:"With Amazon Managed Service for Apache Flink, you can use Java, Scala, Python, or SQL to process and analyze streaming data. The service enables you to author and run code against streaming sources and static sources to perform time-series analytics, feed real-time dashboards, and metrics."}),"\n",(0,s.jsx)(a.p,{children:"Managed Service for Apache Flink provides the underlying infrastructure for your Apache Flink applications. It handles core capabilities like provisioning compute resources, AZ failover resilience, parallel computation, automatic scaling, and application backups (implemented as checkpoints and snapshots). You can use the high-level Flink programming features (such as operators, functions, sources, and sinks) in the same way that you use them when hosting the Flink infrastructure yourself."}),"\n",(0,s.jsx)(a.p,{children:(0,s.jsx)(a.img,{alt:"Amazon Managed Service for Apache Flink",src:t(322619).A+"",width:"1204",height:"914"})}),"\n",(0,s.jsx)(a.p,{children:(0,s.jsx)(a.a,{href:"https://aws.amazon.com/managed-service-apache-flink/",children:"Stream Processing - Amazon Managed Service for Apache Flink - AWS"})}),"\n",(0,s.jsx)(a.p,{children:(0,s.jsx)(a.a,{href:"https://docs.aws.amazon.com/managed-flink/latest/java/what-is.html",children:"What is Amazon Managed Service for Apache Flink? - Managed Service for Apache Flink"})}),"\n",(0,s.jsx)(a.h2,{id:"links-1",children:"Links"}),"\n",(0,s.jsxs)(a.ul,{children:["\n",(0,s.jsx)(a.li,{children:(0,s.jsx)(a.a,{href:"/technologies/apache-spark/06-sliding-window-analytics",children:"06-sliding-window-analytics"})}),"\n",(0,s.jsx)(a.li,{children:(0,s.jsx)(a.a,{href:"https://developer.confluent.io/courses/flink-sql/overview/",children:"Apache Flink\xae SQL"})}),"\n",(0,s.jsx)(a.li,{children:(0,s.jsx)(a.a,{href:"https://www.youtube.com/playlist?list=PLf38f5LhQthefDFLQwHXdLmFsrZWUQWbw",children:"Apache Flink\xae SQL - YouTube"})}),"\n",(0,s.jsx)(a.li,{children:(0,s.jsx)(a.a,{href:"https://dzone.com/articles/introduction-to-streaming-etl-with-apache-flink",children:"https://dzone.com/articles/introduction-to-streaming-etl-with-apache-flink"})}),"\n",(0,s.jsx)(a.li,{children:(0,s.jsx)(a.a,{href:"https://flink.apache.org/flink-architecture.html",children:"https://flink.apache.org/flink-architecture.html"})}),"\n",(0,s.jsx)(a.li,{children:(0,s.jsx)(a.a,{href:"https://youtu.be/K2ibvfmFh8Y?si=_K1jRSc0ez7Ntw9y",children:"CDC Stream Processing with Apache Flink"})}),"\n",(0,s.jsx)(a.li,{children:(0,s.jsx)(a.a,{href:"https://youtu.be/362g8odTRYk",children:"Apache Flink 1.19 - Deprecations, New Features, and Improvements"})}),"\n",(0,s.jsx)(a.li,{children:(0,s.jsx)(a.a,{href:"https://aws.amazon.com/blogs/big-data/krones-real-time-production-line-monitoring-with-amazon-managed-service-for-apache-flink/",children:"Krones real-time production line monitoring with Amazon Managed Service for Apache Flink | AWS Big Data Blog"})}),"\n",(0,s.jsx)(a.li,{children:(0,s.jsx)(a.a,{href:"https://youtu.be/PUmRMf8vqrE",children:"Welcome to Amazon Managed Service for Apache Flink learning series | Amazon Web Services - YouTube"})}),"\n",(0,s.jsx)(a.li,{children:(0,s.jsx)(a.a,{href:"https://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/concepts/dynamic_tables/",children:"Dynamic Tables | Apache Flink"})}),"\n",(0,s.jsx)(a.li,{children:(0,s.jsx)(a.a,{href:"https://youtu.be/PVoc5tRr6to",children:"What is Apache Flink\xae?"})}),"\n",(0,s.jsx)(a.li,{children:(0,s.jsx)(a.a,{href:"https://youtu.be/gAIOZiJVECg",children:"Apache Kafka Vs. Apache Flink"})}),"\n",(0,s.jsx)(a.li,{children:(0,s.jsx)(a.a,{href:"https://youtu.be/fYO5-6Owt0w",children:"Apache Flink - A Must-Have For Your Streams | Systems Design Interview 0..."})}),"\n",(0,s.jsx)(a.li,{children:(0,s.jsx)(a.a,{href:"https://youtu.be/V3Q3EkbEc_k",children:"Apache Spark Vs. Apache Flink Vs. Apache Kafka Vs. Apache Storm! Data St..."})}),"\n",(0,s.jsx)(a.li,{children:(0,s.jsx)(a.a,{href:"https://youtu.be/LYf05ArIkzA",children:"Stateless vs Stateful Stream Processing with Kafka Streams and Apache Flink"})}),"\n",(0,s.jsx)(a.li,{children:(0,s.jsx)(a.a,{href:"https://youtu.be/JfqoVuVDYUE",children:"Consume Apache Kafka Messages using Apache Flink and Java"})}),"\n"]})]})}function d(e={}){const{wrapper:a}={...(0,i.R)(),...e.components};return a?(0,s.jsx)(a,{...e,children:(0,s.jsx)(h,{...e})}):h(e)}},322619:(e,a,t)=>{t.d(a,{A:()=>n});const n=t.p+"assets/images/Screenshot 2025-09-18 at 4.00.38 PM-2b7396f4afc014f2c40fd228d9b91799.jpg"},344267:(e,a,t)=>{t.d(a,{A:()=>n});const n=t.p+"assets/images/Technologies-Apache-Others-image1-c5b843723a7d5b09f50530e70044dc8d.jpg"},28453:(e,a,t)=>{t.d(a,{R:()=>r,x:()=>o});var n=t(296540);const s={},i=n.createContext(s);function r(e){const a=n.useContext(i);return n.useMemo((function(){return"function"==typeof e?e(a):{...a,...e}}),[a,e])}function o(e){let a;return a=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:r(e.components),n.createElement(i.Provider,{value:a},e.children)}}}]);