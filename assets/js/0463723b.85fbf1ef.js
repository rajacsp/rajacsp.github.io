"use strict";(self.webpackChunkdeep_notes=self.webpackChunkdeep_notes||[]).push([[65326],{856775:(e,a,t)=>{t.r(a),t.d(a,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>i,metadata:()=>n,toc:()=>c});const n=JSON.parse('{"id":"technologies/kafka/warpstream","title":"WarpStream","description":"Kafka is dead, long live Kafka","source":"@site/docs/technologies/kafka/warpstream.md","sourceDirName":"technologies/kafka","slug":"/technologies/kafka/warpstream","permalink":"/technologies/kafka/warpstream","draft":false,"unlisted":false,"editUrl":"https://github.com/rajacsp/rajacsp.github.io/tree/master/docs/technologies/kafka/warpstream.md","tags":[],"version":"current","lastUpdatedBy":"Deepak","lastUpdatedAt":1758482757000,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Strimzi","permalink":"/technologies/kafka/strimzi"},"next":{"title":"Others","permalink":"/technologies/others/"}}');var r=t(474848),s=t(28453);const i={},o="WarpStream",l={},c=[{value:"WarpStream Tableflow",id:"warpstream-tableflow",level:2},{value:"Links",id:"links",level:2}];function d(e){const a={a:"a",h1:"h1",h2:"h2",header:"header",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(a.header,{children:(0,r.jsx)(a.h1,{id:"warpstream",children:"WarpStream"})}),"\n",(0,r.jsx)(a.p,{children:(0,r.jsx)(a.a,{href:"https://www.warpstream.com/blog/kafka-is-dead-long-live-kafka",children:"Kafka is dead, long live Kafka"})}),"\n",(0,r.jsxs)(a.p,{children:["WarpStream is an\xa0Apache Kafka protocol ",(0,r.jsx)(a.strong,{children:"compatible data streaming platform"})," built directly on top of ",(0,r.jsx)(a.strong,{children:"S3"}),". It's delivered as a single, stateless Go binary, so there are no local disks to manage, no brokers to rebalance, and no ZooKeeper to operate. WarpStream is 5-10x cheaper than Kafka in the cloud because data streams directly to and from S3 instead of using inter-zone networking, which can be over 80% of the infrastructure cost of a Kafka deployment at scale."]}),"\n",(0,r.jsx)(a.h2,{id:"warpstream-tableflow",children:"WarpStream Tableflow"}),"\n",(0,r.jsx)(a.p,{children:(0,r.jsx)(a.strong,{children:"It reads from Kafka, builds Iceberg tables, and keeps them compacted"})}),"\n",(0,r.jsxs)(a.p,{children:["Tableflow automates\xa0",(0,r.jsx)(a.strong,{children:"all"}),"\xa0of the annoying parts about generating and maintaining Iceberg tables:"]}),"\n",(0,r.jsxs)(a.ol,{children:["\n",(0,r.jsx)(a.li,{children:"It auto-scales."}),"\n",(0,r.jsx)(a.li,{children:"It integrates with schema registries or lets you declare the schemas inline."}),"\n",(0,r.jsx)(a.li,{children:"It has a DLQ."}),"\n",(0,r.jsx)(a.li,{children:"It handles upserts."}),"\n",(0,r.jsx)(a.li,{children:"It enforces retention policies."}),"\n",(0,r.jsx)(a.li,{children:"It can perform stateless transformations as records are ingested."}),"\n",(0,r.jsx)(a.li,{children:"It keeps the table compacted, and it does so continuously and incrementally without having to run a giant major compaction at regular intervals."}),"\n",(0,r.jsx)(a.li,{children:"It cleans up old snapshots automatically."}),"\n",(0,r.jsx)(a.li,{children:"It detects and cleans up orphaned files that were created as part of failed inserts or compactions."}),"\n",(0,r.jsx)(a.li,{children:"It can ingest data at massive rates (GiBs/s) while also maintaining strict (and configurable) freshness guarantees."}),"\n",(0,r.jsx)(a.li,{children:"It speaks multiple table formats (yes, Delta lake too)."}),"\n",(0,r.jsx)(a.li,{children:"It works exactly the same in every cloud."}),"\n"]}),"\n",(0,r.jsx)(a.p,{children:(0,r.jsx)(a.a,{href:"https://www.warpstream.com/blog/the-case-for-an-iceberg-native-database-why-spark-jobs-and-zero-copy-kafka-wont-cut-it",children:"The Case for an Iceberg-Native Database: Why Spark Jobs and Zero-Copy Kafka Won\u2019t Cut It - WarpStream"})}),"\n",(0,r.jsx)(a.h2,{id:"links",children:"Links"}),"\n",(0,r.jsxs)(a.p,{children:["Confluent acquired WarpStream for\xa0",(0,r.jsx)(a.strong,{children:"$220 million"}),"\xa0in a deal completed on September 9, 2024. This acquisition was reported in cash and stock, bringing WarpStream's technology and talent to the data streaming platform Confluent"]}),"\n",(0,r.jsxs)(a.ul,{children:["\n",(0,r.jsx)(a.li,{children:(0,r.jsx)(a.a,{href:"https://www.warpstream.com/",children:"WarpStream - An Apache Kafka Compatible Data Streaming Platform"})}),"\n",(0,r.jsx)(a.li,{children:(0,r.jsx)(a.a,{href:"https://www.youtube.com/watch?v=J1pwvHToOhg",children:"Intro to WarpStream in 5 Minutes - YouTube"})}),"\n",(0,r.jsx)(a.li,{children:(0,r.jsx)(a.a,{href:"https://www.youtube.com/watch?v=NcH4jDyJECY",children:"How WarpStream reinvented Kafka (and soared to a $220m exit in only 13 months) - YouTube"})}),"\n",(0,r.jsx)(a.li,{children:(0,r.jsx)(a.a,{href:"https://hackernoon.com/what-the-heck-is-warpstream",children:"What The Heck is WarpStream? | HackerNoon"})}),"\n",(0,r.jsx)(a.li,{children:(0,r.jsx)(a.a,{href:"https://www.confluent.io/blog/confluent-acquires-warpstream/",children:"Confluent acquires WarpStream | Confluent"})}),"\n",(0,r.jsx)(a.li,{children:(0,r.jsx)(a.a,{href:"https://www.warpstream.com/blog/warpstream-is-dead-long-live-warpstream",children:"WarpStream is Dead, Long Live WarpStream"})}),"\n"]})]})}function h(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,r.jsx)(a,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}},28453:(e,a,t)=>{t.d(a,{R:()=>i,x:()=>o});var n=t(296540);const r={},s=n.createContext(r);function i(e){const a=n.useContext(s);return n.useMemo((function(){return"function"==typeof e?e(a):{...a,...e}}),[a,e])}function o(e){let a;return a=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:i(e.components),n.createElement(s.Provider,{value:a},e.children)}}}]);