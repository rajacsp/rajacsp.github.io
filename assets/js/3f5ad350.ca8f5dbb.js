"use strict";(self.webpackChunkdeep_notes=self.webpackChunkdeep_notes||[]).push([[48598],{107345:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>o,contentTitle:()=>l,default:()=>h,frontMatter:()=>a,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"about-raja-csp/projects/39-ai-powered-call-quality-monitoring","title":"AI Powered Call Quality Monitoring","description":"Overview","source":"@site/docs/about-raja-csp/projects/39-ai-powered-call-quality-monitoring.md","sourceDirName":"about-raja-csp/projects","slug":"/about-raja-csp/projects/39-ai-powered-call-quality-monitoring","permalink":"/about-raja-csp/projects/39-ai-powered-call-quality-monitoring","draft":false,"unlisted":false,"editUrl":"https://github.com/rajacsp/rajacsp.github.io/tree/master/docs/about-raja-csp/projects/39-ai-powered-call-quality-monitoring.md","tags":[],"version":"current","lastUpdatedBy":"Raja CSP Raman","lastUpdatedAt":1765095556000,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Data Stack Evaluation & Optimization","permalink":"/about-raja-csp/projects/38-data-stack-evaluation-optimization"},"next":{"title":"On-Premise MLOps Platform","permalink":"/about-raja-csp/projects/40-on-premise-mlops-platform"}}');var t=i(474848),r=i(28453);const a={},l="AI Powered Call Quality Monitoring",o={},c=[{value:"Overview",id:"overview",level:2},{value:"High-Level Architecture Diagram",id:"high-level-architecture-diagram",level:2},{value:"Components",id:"components",level:2},{value:"Technical Details",id:"technical-details",level:2},{value:"Data Flow",id:"data-flow",level:3},{value:"Key AWS Services",id:"key-aws-services",level:3},{value:"Features",id:"features",level:2},{value:"Call Characteristics",id:"call-characteristics",level:3},{value:"Generative Summarization",id:"generative-summarization",level:3},{value:"Toxic Speech Detection",id:"toxic-speech-detection",level:3},{value:"Scalability &amp; Performance",id:"scalability--performance",level:3},{value:"Security",id:"security",level:3},{value:"Conclusion",id:"conclusion",level:2},{value:"Call Transcribing",id:"call-transcribing",level:2},{value:"Purpose",id:"purpose",level:3},{value:"Benefits",id:"benefits",level:3},{value:"Quality assurance process",id:"quality-assurance-process",level:3},{value:"Real-Time Factor (RTF)",id:"real-time-factor-rtf",level:3},{value:"Tools",id:"tools",level:2},{value:"Speech to text Model - Whisper / Voice to Text / Audio to Text",id:"speech-to-text-model---whisper--voice-to-text--audio-to-text",level:2},{value:"Text to Voice",id:"text-to-voice",level:2},{value:"GitHub - freddyaboulton/fastrtc: The python library for real-time communication",id:"github---freddyaboultonfastrtc-the-python-library-for-real-time-communication",level:3}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"ai-powered-call-quality-monitoring",children:"AI Powered Call Quality Monitoring"})}),"\n",(0,t.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,t.jsx)(n.p,{children:"This solution document details the architecture and technical implementation of an AI-powered call quality monitoring system. The system provides end-to-end capabilities to process call recordings from existing contact centers, delivering actionable insights such as sentiment analysis, call transcription, categorization, and post-call analytics."}),"\n",(0,t.jsx)(n.h2,{id:"high-level-architecture-diagram",children:"High-Level Architecture Diagram"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"post-call-analytics-high-level-architecture-diagram",src:i(700376).A+"",width:"1626",height:"1130"})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.a,{href:"https://github.com/aws-samples/amazon-transcribe-post-call-analytics",children:"GitHub - aws-samples/amazon-transcribe-post-call-analytics"})}),"\n",(0,t.jsx)(n.h2,{id:"components",children:"Components"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"post-call-analytics-components-diagram",src:i(429147).A+"",width:"2165",height:"1546"})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.a,{href:"https://drive.google.com/file/d/1m1S1eTfrySq2AauD4gNb5eByDW9pyfxU/view?usp=drive_link",children:"https://drive.google.com/file/d/1m1S1eTfrySq2AauD4gNb5eByDW9pyfxU/view?usp=drive_link"})}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Source Input Data"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Audio Files: Delivered to an ingestion location in Amazon S3."}),"\n",(0,t.jsx)(n.li,{children:"Transcript Files: Generated by Amazon Transcribe and stored in S3."}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Transcription"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Batch processing of audio files using Amazon Transcribe."}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Features include:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Custom vocabulary for domain-specific terminology."}),"\n",(0,t.jsx)(n.li,{children:"PII redaction and vocabulary filtering."}),"\n",(0,t.jsx)(n.li,{children:"Multi-language support with automatic detection."}),"\n",(0,t.jsx)(n.li,{children:"Caller and agent speaker labels using speaker diarization or channel identification."}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Data Processing & Enrichment"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Sentiment Analysis:"})," Detects caller and agent sentiment trends."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Talk & Non-Talk Time:"})," Measures speaking and silence intervals."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Interruption Detection:"})," Identifies overlapping speech."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Entity Detection:"})," Uses Amazon Comprehend for extracting entities."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Loudness Analysis:"})," Normalized loudness metrics for both parties."]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Analytics Engine"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["Provides insights such as:","\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Sentiment trends."}),"\n",(0,t.jsx)(n.li,{children:"Call categorization based on keywords, sentiment, and interruptions."}),"\n",(0,t.jsx)(n.li,{children:"Issue detection using pre-built ML models."}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.li,{children:"Generates summaries for key call information."}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Search Index"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Indexes call attributes such as time range, sentiment, entities, and transcription for efficient search capabilities."}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Dashboards & Reporting"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Visualizations include:"}),"\n",(0,t.jsx)(n.li,{children:"Call trends (sentiment, loudness, interruptions)."}),"\n",(0,t.jsx)(n.li,{children:"Training insights for quality assurance."}),"\n",(0,t.jsx)(n.li,{children:"Adherence to compliance standards."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"technical-details",children:"Technical Details"}),"\n",(0,t.jsx)(n.h3,{id:"data-flow",children:"Data Flow"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Ingestion"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Audio and transcript files are uploaded to Amazon S3 buckets."}),"\n",(0,t.jsx)(n.li,{children:"Notifications (using S3 events) trigger processing workflows via AWS Lambda or Step Functions."}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Transcription"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Audio files are processed using Amazon Transcribe."}),"\n",(0,t.jsx)(n.li,{children:"Transcriptions are stored in S3 for further processing."}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Processing Pipeline"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"AWS Glue processes transcripts and audio metadata."}),"\n",(0,t.jsx)(n.li,{children:"Sentiment analysis and entity detection are performed using Amazon Comprehend."}),"\n",(0,t.jsx)(n.li,{children:"Custom categorization is applied based on business rules."}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Storage"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Processed data is stored in Amazon RDS or DynamoDB for structured queries."}),"\n",(0,t.jsx)(n.li,{children:"Elasticsearch or OpenSearch is used for indexing and search."}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Analytics & Reporting"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Amazon QuickSight provides dashboards for real-time monitoring."}),"\n",(0,t.jsx)(n.li,{children:"Reports can be exported to PDF or CSV formats."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"key-aws-services",children:"Key AWS Services"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Amazon S3:"})," For audio and transcript storage."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Amazon Transcribe:"})," For audio-to-text conversion."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Amazon Comprehend:"})," For NLP tasks such as sentiment and entity analysis."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Amazon RDS/DynamoDB:"})," For structured data storage."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Amazon OpenSearch:"})," For search indexing and querying."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"AWS Glue:"})," For ETL processes."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Amazon QuickSight:"})," For analytics and reporting."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"AWS Lambda:"})," For event-driven processing."]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"features",children:"Features"}),"\n",(0,t.jsx)(n.h3,{id:"call-characteristics",children:"Call Characteristics"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Interruption Detection:"})," Identifies interruptions during calls."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Talk Time & Speed:"})," Measures speech duration and words per minute."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Loudness Analysis:"})," Detects yelling or speaking softly."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Non-Talk Time:"})," Tracks periods of silence."]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"generative-summarization",children:"Generative Summarization"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Automatically summarizes calls, highlighting key issues, resolutions, and next steps."}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"toxic-speech-detection",children:"Toxic Speech Detection"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Flags abusive or harmful speech using pitch, tone, and content analysis."}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"scalability--performance",children:"Scalability & Performance"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Auto-scaling:"})," Leverages AWS Lambda and Step Functions for variable call volumes."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Data Partitioning:"})," Ensures performance across large datasets using DynamoDB partitioning."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Monitoring:"})," Uses AWS CloudWatch for real-time system monitoring."]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"security",children:"Security"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"PII Redaction:"})," Ensures sensitive information is removed from transcripts and audio."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Encryption:"})," S3 buckets and databases use server-side encryption."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Access Control:"})," Managed via IAM roles and policies."]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,t.jsx)(n.p,{children:"This AI-powered call quality monitoring system provides a robust, scalable, and secure solution for deriving actionable insights from call data. Leveraging AWS services, it ensures efficient processing, accurate analytics, and seamless integration with existing contact center workflows."}),"\n",(0,t.jsx)(n.h2,{id:"call-transcribing",children:"Call Transcribing"}),"\n",(0,t.jsx)(n.p,{children:'"Call transcribing" refers to the process of converting a recorded phone conversation into written text, while "quality assurance" in this context means the practice of reviewing those transcribed calls to ensure accuracy and adherence to quality standards, often used to evaluate customer service interactions and agent performance within a company.'}),"\n",(0,t.jsx)(n.p,{children:"Key points about call transcribing and quality assurance:"}),"\n",(0,t.jsx)(n.h3,{id:"purpose",children:"Purpose"}),"\n",(0,t.jsx)(n.p,{children:"Companies often record customer service calls for quality assurance, which involves transcribing the conversation to review details like agent responses, issue resolution, and adherence to company policies."}),"\n",(0,t.jsx)(n.h3,{id:"benefits",children:"Benefits"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Agent training:"})," Transcripts can be used to identify areas where agents need improvement in communication skills or product knowledge."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Customer experience evaluation:"})," Analyzing transcripts allows companies to assess customer satisfaction and identify potential issues."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Compliance checks:"})," In industries with strict regulations, call transcripts can be used to verify compliance with legal requirements."]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"quality-assurance-process",children:"Quality assurance process"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Sampling:"})," A representative sample of calls is selected for transcription."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Transcription:"})," The audio is converted into written text, ensuring accuracy and capturing key details like pauses and tone of voice."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Review and evaluation:"})," Quality assurance specialists review the transcripts against established criteria, assessing aspects like agent greetings, problem-solving techniques, and overall professionalism."]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"real-time-factor-rtf",children:"Real-Time Factor (RTF)"}),"\n",(0,t.jsxs)(n.p,{children:["The ",(0,t.jsx)(n.strong,{children:"real-time factor (RTF)"})," is the ratio of the processing (or transcription) time to the actual duration of the audio. In other words, it measures how fast a system processes audio relative to real time. An RTF less than 1 means the system is faster than real time."]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Example:"})}),"\n",(0,t.jsx)(n.p,{children:"Suppose an AI tool transcribes a 1\u2011minute (60\u2011second) call in 1 second. Here, the RTF is:"}),"\n",(0,t.jsx)(n.p,{children:"\u2003\u2003RTF = Processing Time / Audio Duration = 1 sec / 60 sec = 1/60"}),"\n",(0,t.jsx)(n.p,{children:"This indicates that the system is 60 times faster than real time. If you have a call lasting x minutes and the system transcribes it in x seconds, the RTF remains 1/60, meaning it delivers the transcript at 60\xd7 real-time speed."}),"\n",(0,t.jsx)(n.p,{children:"This fast turnaround is particularly valuable in call quality monitoring, where near real\u2011time feedback can help promptly address issues or monitor performance."}),"\n",(0,t.jsx)(n.h2,{id:"tools",children:"Tools"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"/cloud/aws/ai/amazon-transcribe",children:"Amazon Transcribe"})}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.a,{href:"https://voxjar.com/",children:"AI for Call Center Quality Assurance | Voxjar"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://app.voxjar.com/dashboard",children:"app.voxjar.com/dashboard"})}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.a,{href:"https://convin.ai/",children:"Convin: Omnichannel Contact Centers Powered By Conversation Intelligence"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://convin.ai/news-collection/g2-rank-speech-analytics-category",children:"Kicking Off 2024 on a High: Convin Ranked as G2's #1 Speech Analytics Solution"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://convin.ai/products/call-center-monitoring-software",children:"Call Center Monitoring Software | Convin"})}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://thelevel.ai/quality-assurance-contact-center/",children:"Quality Assurance Software for Contact and Call Centers"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://trellissoft.ai/products/chatterscore/",children:"Call Quality Monitoring Software with 100% AI | Chatterscore"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://callcenterstudio.com/blog/ai-powered-quality-management-and-performance-monitoring-in-call-centers/",children:"AI-Powered Quality Management and Performance Monitoring in Call Centers 1"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://enthu.ai/blog/call-center-quality-monitoring-software/",children:"10 Best Call Monitoring Software in 2024 - Enthu AI"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://github.com/jiaaro/pydub",children:"GitHub - jiaaro/pydub: Manipulate audio with a simple and easy high level interface"})}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"speech-to-text-model---whisper--voice-to-text--audio-to-text",children:"Speech to text Model - Whisper / Voice to Text / Audio to Text"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://huggingface.co/openai/whisper-large-v3",children:"openai/whisper-large-v3 \xb7 Hugging Face"})}),"\n",(0,t.jsx)(n.li,{children:"Whisper is a general-purpose speech recognition model. It is trained on a large dataset of diverse audio and is also a multitasking model that can perform multilingual speech recognition, speech translation, and language identification."}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://github.com/openai/whisper",children:"GitHub - openai/whisper: Robust Speech Recognition via Large-Scale Weak Supervision"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://github.com/petewarden/spchcat",children:"GitHub - petewarden/spchcat: Speech recognition tool to convert audio to text transcripts, for Linux and Raspberry Pi."})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://zapier.com/blog/best-text-dictation-software/",children:"The best dictation and speech-to-text software | Zapier"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://www.regal.ai/",children:"REGAL | The AI Agent Platform"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://livekit.io/",children:"LiveKit"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://demo.gigaml.com/",children:"Demo | GigaML"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://www.teneo.ai/",children:"Teneo.ai | Make Your AI Agent the Smartest"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://www.voiceflow.com/",children:"Build Chat and Voice AI Agents Without Code | Voiceflow"})}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"text-to-voice",children:"Text to Voice"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://voicemaker.in/",children:"Voicemaker\xae - Text to Speech Converter"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://elevenlabs.io/",children:"Bring Agents and Creative to Life with our AI Voice Generator"})}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"github---freddyaboultonfastrtc-the-python-library-for-real-time-communication",children:(0,t.jsx)(n.a,{href:"https://github.com/freddyaboulton/fastrtc",children:"GitHub - freddyaboulton/fastrtc: The python library for real-time communication"})}),"\n",(0,t.jsx)(n.p,{children:"Turn any python function into a real-time audio and video stream over WebRTC or WebSockets."}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"\ud83d\udde3\ufe0f Automatic Voice Detection and Turn Taking built-in, only worry about the logic for responding to the user."}),"\n",(0,t.jsxs)(n.li,{children:["\ud83d\udcbb Automatic UI - Use the\xa0",(0,t.jsx)(n.code,{children:".ui.launch()"}),"\xa0method to launch the webRTC-enabled built-in Gradio UI."]}),"\n",(0,t.jsxs)(n.li,{children:["\ud83d\udd0c Automatic WebRTC Support - Use the\xa0",(0,t.jsx)(n.code,{children:".mount(app)"}),"\xa0method to mount the stream on a FastAPI app and get a webRTC endpoint for your own frontend!"]}),"\n",(0,t.jsxs)(n.li,{children:["\u26a1\ufe0f Websocket Support - Use the\xa0",(0,t.jsx)(n.code,{children:".mount(app)"}),"\xa0method to mount the stream on a FastAPI app and get a websocket endpoint for your own frontend!"]}),"\n",(0,t.jsxs)(n.li,{children:["\ud83d\udcde Automatic Telephone Support - Use the\xa0",(0,t.jsx)(n.code,{children:"fastphone()"}),"\xa0method of the stream to launch the application and get a free temporary phone number!"]}),"\n",(0,t.jsxs)(n.li,{children:["\ud83e\udd16 Completely customizable backend - A\xa0",(0,t.jsx)(n.code,{children:"Stream"}),"\xa0can easily be mounted on a FastAPI app so you can easily extend it to fit your production application. See the\xa0",(0,t.jsx)(n.a,{href:"https://huggingface.co/spaces/fastrtc/talk-to-claude",children:"Talk To Claude"}),"\xa0demo for an example on how to serve a custom JS frontend."]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},700376:(e,n,i)=>{i.d(n,{A:()=>s});const s=i.p+"assets/images/Screenshot 2025-01-25 at 2.48.28 PM-3449771a1465dd63fee4ea480210bc04.jpg"},429147:(e,n,i)=>{i.d(n,{A:()=>s});const s=i.p+"assets/images/post_call_analytics.drawio-752e49bebe0fa11e02aff5b654e61b9f.png"},28453:(e,n,i)=>{i.d(n,{R:()=>a,x:()=>l});var s=i(296540);const t={},r=s.createContext(t);function a(e){const n=s.useContext(r);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:a(e.components),s.createElement(r.Provider,{value:n},e.children)}}}]);